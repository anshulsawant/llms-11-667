\documentclass{article}

\usepackage[margin=1in]{geometry}

\usepackage{xcolor}
\usepackage{titlesec}
\usepackage[scaled=0.96]{helvet} % Sets up Helvetica font
\definecolor{mauve}{rgb}{0.58,0,0.82}

\titleformat{\section}[hang]{\color{mauve}\normalfont\sffamily\Large}{\thesection}{1em}{}
\titleformat{\subsection}[hang]{\color{mauve}\normalfont\sffamily\large}{\thesubsection}{1em}{}


\title{Homework 1}
\author{Anshul Sawant}
\date{Due date: January 24 at 11:59 PM}

\begin{document}
\maketitle

\section*{Question 1.1 (5 points)}
Weight tying makes the input embedding projection (1-hot $\rightarrow$ embedding) and the output unembedding projection (embedding dim $\rightarrow$ vocab size logits) transpose of each other by sharing weights between corresponding matrices. This leads to shared weights being updated at each time step (because gradients flow back from all unsaturated logits) instead of only the input rows for current input tokens being updated. In practice, it leads to faster convergence of input embeddings and consequently faster overall convergence.
\section*{Question 1.2 (5 points)}

\verb|   self.token_logits = nn.Linear(n_embd, vocab_size)|

\verb|self.token_logits.weight = self.token_embeddings.weight|

\end{document}
