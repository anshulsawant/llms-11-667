# configs/config_grpo_debug.yaml
# DEBUG configuration for GRPO training (CPU Version)
# Aims for fast iteration with minimal data/steps on CPU.

# Model & Tokenizer Configuration (Update paths as needed)
defaults:
  - configs/config.yaml # Relative path of default config (wrt project root)

# Override Model & Tokenizer for the tiny debug model
model:
  name: "sbintuitions/tiny-lm-chat" # Use the specified tiny model
  tokenizer_name: "sbintuitions/tiny-lm-chat"
  trust_remote_code: false # Likely not needed for this model based on HF card
  torch_dtype: "float32" # Use float32 for CPU

# Dataset & Task Configuration
dataset:
  name: "gsm8k"
  config: "main"
  split: "train"
  prompt_format: "Question: {question}\nAnswer:"
  max_prompt_length: 512
  max_gen_length: 768 # Max possible length, actual generation limit below

# GRPO Specific Configuration
grpo:
  group_size: 2 # Minimal group size for faster generation

# Shared RL Hyperparameters (Reusing 'ppo' block name for convenience)
ppo:
  learning_rate: 5.0e-7   # Keep low LR for stability even in debug
  epochs: 1             # Only 1 optimization epoch per rollout for speed
  batch_size: 2           # Rollout batch size (prompts) - SMALL
  mini_batch_size: 2      # Update mini-batch size (prompts) - SMALL
  gradient_accumulation_steps: 1 # Effective update batch size (prompts) = 4 - SMALL
  kl_coeff: 0.05          # Keep KL penalty for stability check
  clip_ratio: 0.2         # GRPO policy objective clipping
  entropy_coeff: 0.01     # Keep entropy bonus
  gamma: 0.99             # Discount factor (less critical in GRPO but kept)
  use_8bit_adam: false    # Disable 8-bit Adam for CPU
  max_grad_norm: 1.0
  rollout_samples: 2     # Number of *prompts* per rollout - VERY SMALL
  
  scheduler: "cosine"
  warmup_steps: 2         # Minimal warmup steps for short run
  min_lr: 1.0e-7

# Training Control
training:
  total_ppo_steps: 2     # Very few GRPO steps for quick test
  seed: 42
  log_interval: 1         # Log every step
  save_interval: 100      # Don't save checkpoints during short debug run
  output_dir: "outputs/grpo_debug_cpu_${model.name}" # Specific CPU debug output dir
  device: "cpu"           # Set device to CPU
  gradient_checkpointing: false # Disable (not typically used/effective on CPU)

# Generation settings for rollouts
generation:
  max_new_tokens: 256     # Reduced max generation length for faster rollouts
  min_new_tokens: 5
  temperature: 0.7
  top_k: 50
  top_p: 0.95

# WandB Logging Configuration
wandb:
  report_to_wandb: false  # Disable WandB for debug runs
  project: "tinier-zero-grpo-debug" # Specific debug project name
  name: null              # Auto-generate run name if needed
