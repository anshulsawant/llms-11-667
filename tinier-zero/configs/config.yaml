# configs/config.yaml
# Model & Tokenizer Configuration
model:
  name: "../hw12/sft_results_full_main/final_checkpoint" ## "google/gemma-2b-it"
  tokenizer_name: "../hw12/sft_results_full_main/final_checkpoint" ## "google/gemma-2b-it"
  trust_remote_code: true # Needed for Qwen
  torch_dtype: "bfloat16"

# Dataset & Task Configuration
dataset:
  name: "gsm8k"
  config: "main"
  split: "train" # Use train split for prompts
  prompt_format: "Question: {question}\nAnswer:" # How to format the prompt
  max_prompt_length: 512 # Max tokens for prompt
  max_gen_length: 768  # Max tokens to generate for answer

# PPO Hyperparameters
ppo:
  learning_rate: 1.0e-6
  epochs: 2        # Optimization epochs per rollout phase
  batch_size: 4     # Rollout batch size (adjust based on VRAM)
  mini_batch_size: 4 # Update mini-batch size (adjust based on VRAM)
  gradient_accumulation_steps: 4 # Effective update batch size = mini_batch_size * grad_acc_steps
  kl_coeff: 0.05     # KL penalty coefficient (beta)
  clip_ratio: 0.2   # PPO policy objective clipping
  clip_range_value: 0.2 # PPO value function clipping
  vf_coeff: 0.1     # Value function loss weight
  entropy_coeff: 0.01 # Entropy bonus weight
  gamma: 0.99        # Discount factor
  lam: 0.95         # GAE lambda
  use_8bit_adam: true
  max_grad_norm: 1.0
  rollout_samples: 512

# Training Control
training:
  total_ppo_steps: 100 # Number of PPO steps (Rollout -> Update)
  seed: 42
  log_interval: 1   # Log metrics every N PPO steps
  save_interval: 10 # Save model every N PPO steps
  output_dir: "outputs/ppo_gsm8k_${model.name}" # Sanitize model name, use model name in output
  device: "cuda"    # Set to "cpu" for CPU execution (use with debug config)
  gradient_checkpointing: true

# Generation settings for rollouts
generation:
  max_new_tokens: ${dataset.max_gen_length} # Reference value from dataset section
  min_new_tokens: 5
  temperature: 0.7
  top_k: 50
  top_p: 0.95
  do_sample: true

wandb:
  report_to_wandb: true
  project: "tinier-zero"
  name: null 
