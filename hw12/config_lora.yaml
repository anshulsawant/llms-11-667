# config.yaml: Configuration for Supervised Fine-Tuning

# --- New: Select Tuning Method ---
tuning_method: "lora" # Options: "full", "lora"
# config.yaml: Configuration for Supervised Fine-Tuning

tuning_method: "full" # Options: "full", "lora"

model:
  name: "google/gemma-2-9b-it"
  access_token: null
  trust_remote_code: true

dataset:
  name: "gsm8k"
  config_name: "main"
  train_split: "train"
  eval_split: "test[:100]"
  prompt_format: "Question: {question}\nAnswer: "
  response_template: "{answer}"
  max_seq_length: 1024

training:
  output_dir: "./sft_results"
  overwrite_output_dir: true
  num_train_epochs: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  gradient_checkpointing: true
  learning_rate: 1e-5 # Initial learning rate
  weight_decay: 0.01
  # --- Use the specific scheduler type ---
  lr_scheduler_type: "cosine_with_min_lr" # As identified by user
  # --- Use lr_scheduler_kwargs to pass extra args ---
  lr_scheduler_kwargs:
    # Pass the absolute minimum learning rate value
    min_lr: 1e-6 # Example: decays towards 1e-6
  warmup_ratio: 0.03
  logging_strategy: "steps"
  logging_steps: 10
  save_strategy: "steps"
  save_steps: 100
  evaluation_strategy: "steps"
  eval_steps: 100
  save_total_limit: 2
  bf16: true
  fp16: false
  optim: "adamw_bnb_8bit" # Or "adamw_torch" etc.
  seed: 42
  report_to: ["wandb"]
  # max_grad_norm: 1.0 # Optional gradient clipping

lora_config:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  bias: "none"
  task_type: "CAUSAL_LM"

wandb:
  project: "gemma-sft-gsm8k"
  run_name: "gemma-2-9b-it-gsm8k-run-1"
  watch: "gradients"
  log_model: "checkpoint"

evaluation:
  max_new_tokens: 256
  temperature: 0.1
  do_sample: false
  base_model_prompt_strategy: "zero_shot"
  # --- Updated: Shorter One-Shot Example ---
  one_shot_example:
    question: "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?"
    answer: "Natalia sold 48 clips in April.\nIn May, she sold half as many clips as in April, so she sold 48 / 2 = 24 clips.\nAltogether, Natalia sold 48 + 24 = 72 clips.\n#### 72"
