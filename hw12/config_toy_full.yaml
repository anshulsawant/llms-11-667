# config_toy_full.yaml: Overrides for quick E2E testing (Full SFT)
# Merges with the base config.yaml

tuning_method: "full"

model:
  name: "distilgpt2"
  access_token: null
  trust_remote_code: false

dataset:
  train_split: "train" # Use base name
  eval_split: "test"   # Use base name
  # --- Specify subset sizes ---
  num_train_samples: 32
  num_eval_samples: 16

training:
  output_dir: "./sft_results_toy_full"
  num_train_epochs: 1
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 1
  gradient_checkpointing: false
  learning_rate: 5e-5
  lr_scheduler_type: "cosine"
  lr_scheduler_kwargs: null # Ensure no min_lr is passed
  warmup_ratio: 0.1
  logging_steps: 4
  save_steps: 8
  save_total_limit: 1
  bf16: false
  fp16: false
  optim: "adamw_torch"
  report_to: ["none"]

# lora_config is ignored

wandb:
  project: "gemma-sft-gsm8k-test"
  run_name: "toy-distilgpt2-full"
  watch: "false"
  log_model: "false"

evaluation:
  max_new_tokens: 128
  temperature: 0.7
  # one_shot_example: null # Inherits from base or set explicitly if needed
