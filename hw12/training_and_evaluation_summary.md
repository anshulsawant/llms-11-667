# Report: Gemma 2B IT Fine-Tuning for GSM8K

## Introduction

This report summarizes the results of supervised fine-tuning (SFT) experiments performed on the `google/gemma-2b-it` model for the GSM8K mathematical reasoning task. Four main training runs were conducted and evaluated:

1.  **Full SFT - Main Dataset:** Full fine-tuning using the standard GSM8K `main` dataset.
2.  **Full SFT - Socratic Dataset:** Full fine-tuning using the `socratic` variant of the GSM8K dataset.
3.  **LoRA SFT - Main Dataset:** Fine-tuning using LoRA adapters (`r=16`) on the `main` dataset.
4.  **LoRA SFT - Socratic Dataset:** Fine-tuning using LoRA adapters (`r=16`) on the `socratic` dataset.

The evaluation metric reported is Exact Match Accuracy on the GSM8K test set.

## Training Analysis

### Training Convergence

All four training runs appear to have converged successfully, as indicated by the training loss curves which show a generally decreasing trend over the training steps, eventually stabilizing or fluctuating around a lower value compared to the initial steps.

![Training loss][training_loss_comparison.png]

### Training Speed and Throughput

The training throughput (samples per second) comparison shows that the LoRA runs achieved higher throughput than the Full SFT runs.

![Training throughput][throughput_samples_comparison.png]

This aligns with expectations. LoRA updates fewer parameters, reducing the computation needed for the backward pass and optimizer steps. However, as discussed previously, the overall *wall-clock time* difference might be less dramatic than the throughput difference suggests, especially for smaller models like Gemma 2B where the forward pass (common to both methods) constitutes a significant portion of the step time. The final runtime comparison (from the summary table generated by `analyze_logs.py`) provides the concrete time difference.

### Final Training Loss

Comparing the final training loss values reached by each run:

![Training loss][final_loss_comparison.png]

We observe that for both Full SFT and LoRA, the runs trained on the **Socratic dataset achieved a lower final training loss** compared to the runs trained on the Main dataset.

## Evaluation Results

The following table summarizes the Exact Match Accuracy achieved by each fine-tuned model, along with the baseline performance of the pre-trained `google/gemma-2b-it` model (evaluated using a one-shot prompt), on the GSM8K test set (100 samples).

| Run Configuration              | Dataset   | Tuning Method | Exact Match Accuracy (%) |
| :----------------------------- | :-------- | :------------ | :----------------------- |
| Base Model (One-Shot)        | Main      | None (Base)   | 6.0                      |
| **Full SFT - Main** | **Main** | **Full** | **31.0** |
| Full SFT - Socratic          | Socratic  | Full          | 23.0                     |
| **LoRA SFT - Main** | **Main** | **LoRA** | **20.0** |
| LoRA SFT - Socratic          | Socratic  | LoRA          | 11.0                     |

## Key Findings and Discussion

1.  **Lower Training Loss â‰  Better Performance:** This is a key takeaway. While the Socratic dataset runs resulted in lower final training loss values (Full-Socratic: ~0.44, LoRA-Socratic: ~0.59) compared to the Main dataset runs (Full-Main: ~0.50, LoRA-Main: ~0.65), they performed *worse* in the final evaluation (Full-Socratic: 23% vs Full-Main: 31%; LoRA-Socratic: 11% vs LoRA-Main: 20%). This suggests that merely minimizing loss on the training data doesn't guarantee better generalization to the unseen test set. The model might have overfit to the specific style or content of the Socratic dataset, which was less representative of the test set than the Main dataset was.
2.  **LoRA Speed vs. Performance:** The throughput chart confirms LoRA runs process more samples per second. However, in this set of experiments, Full SFT achieved significantly higher accuracy on both datasets compared to LoRA (Main: 31% vs 20%; Socratic: 23% vs 11%). This highlights the trade-off: LoRA offers potential speed/efficiency benefits but may not reach the same peak performance as full fine-tuning, especially with the chosen LoRA configuration (`r=16`, extensive target modules).
3.  **Fine-Tuning Improvement:** Both Full SFT and LoRA significantly improved performance over the base `gemma-2b-it` model's one-shot accuracy (6%). Full SFT on the Main dataset showed the largest improvement (+25% absolute accuracy).

## Conclusion

In conclusion, these experiments confirm that fine-tuning `gemma-2b-it` significantly enhances performance on the GSM8K task compared to the base model. While LoRA demonstrated higher training throughput, Full SFT achieved superior accuracy in these tests. A key observation is the divergence between training loss and evaluation accuracy when comparing the Main and Socratic datasets; the Socratic runs yielded lower training loss but resulted in poorer test set accuracy, highlighting that the training loss metric alone may not perfectly predict performance on the target evaluation task.
