# config.yaml: Configuration for Supervised Fine-Tuning

tuning_method: "full" # Options: "full", "lora"

model:
  # --- Updated model name ---
  name: "google/gemma-2b-it"
  # --- End updated model name ---
  access_token: null
  trust_remote_code: true

dataset:
  name: "gsm8k"
  config_name: "main" # Used in output filename
  train_split: "train"
  eval_split: "test"
  num_train_samples: null
  num_eval_samples: 100
  eval_random_subset: true
  prompt_format: "Question: {question}\nAnswer: "
  response_template: "{answer}"
  max_seq_length: 1024

training:
  output_dir: "./sft_results"
  overwrite_output_dir: true
  num_train_epochs: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  gradient_checkpointing: true
  learning_rate: 1e-5
  weight_decay: 0.01
  lr_scheduler_type: "cosine_with_min_lr"
  lr_scheduler_kwargs:
    min_lr: 1e-6
  warmup_ratio: 0.03
  logging_strategy: "steps"
  logging_steps: 1
  save_strategy: "steps"
  save_steps: 100
  eval_strategy: "no"
  save_total_limit: 2
  bf16: true
  fp16: false
  optim: "adamw_bnb_8bit"
  seed: 42
  report_to: ["wandb"]
  # max_grad_norm: 1.0

lora_config:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  bias: "none"
  task_type: "CAUSAL_LM"

wandb:
  project: "gemma-sft-gsm8k"
  run_name: "gemma-2-9b-it-gsm8k-run-1" # Consider updating this run name
  watch: "gradients"
  log_model: "checkpoint"

evaluation: # Settings used by evaluate.py (and fallback for inference.py)
  max_new_tokens: 256
  temperature: 0.1
  do_sample: true
  base_model_prompt_strategy: "one_shot" # Strategy for base model prompting
  base_model_instruction: "You are a helpful math assistant. Please provide a step-by-step derivation and end your answer with the final numerical result in the format '#### <number>'."
  one_shot_example: # Example used if strategy is one_shot
    question: "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?"
    answer: "Natalia sold 48 clips in April.\nIn May, she sold half as many clips as in April, so she sold 48 / 2 = 24 clips.\nAltogether, Natalia sold 48 + 24 = 72 clips.\n#### 72"

# --- Updated inference section ---
inference:
  # --- Add paths and fields here ---
  input_file: "data/inference_input.jsonl" # Default input file path
  output_dir: "./inference_outputs"      # Default output directory
  prompt_field: "question"                # Default input field name
  output_field: "generation"              # Default output field name
  # --- End added paths/fields ---
  precision: "bf16"
  batch_size: 4
  max_new_tokens: 256
  temperature: 0.1
  do_sample: true
  # --- Base model settings for inference ---
  base_model_instruction: "You are a helpful math assistant. Please provide a step-by-step derivation and end your answer with the final numerical result in the format '#### <number>'."
  base_model_prompt_strategy: "one_shot" # Set strategy explicitly for inference
  one_shot_example: # Add the same example here for inference to use
    question: "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?"
    answer: "Natalia sold 48 clips in April.\nIn May, she sold half as many clips as in April, so she sold 48 / 2 = 24 clips.\nAltogether, Natalia sold 48 + 24 = 72 clips.\n#### 72"
  # --- End base model settings ---
# --- End inference section ---
