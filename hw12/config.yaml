# config.yaml: Configuration for Supervised Fine-Tuning

tuning_method: "full" # Options: "full", "lora"

model:
  name: "google/gemma-2-9b-it"
  access_token: null
  trust_remote_code: true

dataset:
  name: "gsm8k"
  config_name: "main"
  train_split: "train" # Base split name
  eval_split: "test"  # Base split name
  # --- Optional subset sizes ---
  num_train_samples: null # Use null or omit to use full train split
  num_eval_samples: 100   # Number of eval samples to select
  # --- Control how eval subset is selected ---
  eval_random_subset: true # Set to true to randomly sample num_eval_samples
  # --- End eval subset control ---
  prompt_format: "Question: {question}\nAnswer: "
  response_template: "{answer}"
  max_seq_length: 1024

training:
  output_dir: "./sft_results"
  overwrite_output_dir: true
  num_train_epochs: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  gradient_checkpointing: true
  learning_rate: 1e-5
  weight_decay: 0.01
  lr_scheduler_type: "cosine_with_min_lr"
  lr_scheduler_kwargs:
    min_lr: 1e-6
  warmup_ratio: 0.03
  logging_strategy: "steps"
  logging_steps: 10
  save_strategy: "steps"
  save_steps: 100
  evaluation_strategy: "steps"
  eval_steps: 100
  save_total_limit: 2
  bf16: true
  fp16: false
  optim: "adamw_bnb_8bit"
  seed: 42 # Seed used for shuffling if eval_random_subset is true
  report_to: ["wandb"]
  # max_grad_norm: 1.0

lora_config:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  bias: "none"
  task_type: "CAUSAL_LM"

wandb:
  project: "gemma-sft-gsm8k"
  run_name: "gemma-2-9b-it-gsm8k-run-1"
  watch: "gradients"
  log_model: "checkpoint"

evaluation:
  max_new_tokens: 256
  temperature: 0.1
  do_sample: true
  base_model_prompt_strategy: "one_shot"
  one_shot_example:
    question: "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?"
    answer: "Natalia sold 48 clips in April.\nIn May, she sold half as many clips as in April, so she sold 48 / 2 = 24 clips.\nAltogether, Natalia sold 48 + 24 = 72 clips.\n#### 72"
