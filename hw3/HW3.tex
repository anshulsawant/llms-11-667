% Created 2025-01-31 Fri 07:36
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Anshul Sawant}
\date{\textit{<2025-01-25 Sat>}}
\title{Homework 3}
\hypersetup{
 pdfauthor={Anshul Sawant},
 pdftitle={Homework 3},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.4 (Org mode 9.6.15)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\section{Homework 3}
\label{sec:org0c0026f}
\subsection{Question 1.2}
\label{sec:org157fc3e}
\subsubsection{All the Generations}
\label{sec:org4d41dfb}

\textbf{Prefixes}
\begin{enumerate}
\item In the heart of the jungle
\item The little red fox
\item The year is 2342. Humanity has
\end{enumerate}


\textbf{Generations of our model (\texttt{T = 0.01, max\_new\_tokens = 32, ppx = 1.025})}
\begin{enumerate}
\item , the world is a great way to get the best of the world.\nThe world is a great way to get the best of the world.\nThe
\item is a little bit of a big black fox.\nThe little red fox is a little bit of a big black fox.\nThe little red fox is a
\item been a great resource for the world.\nThe world is a great resource for the world.\nThe world is a great resource for the world.\nThe
\end{enumerate}


\textbf{Generations of our model (\texttt{T = 0.50, max\_new\_tokens = 32, ppx = 2.93})}
\begin{enumerate}
\item , the world\u2019s most beautiful creatures.\nThe first thing that\u2019s been to be a life-changing world is to be a place where"\}
\item is a red fox that\u2019s a little bit more than that.\nThis is a good idea to keep the tree in the middle of the tree."\}
\item been a long time since the year is a long time since the year is a long time since the year has been a long time since the year is a long"\}
\end{enumerate}


\textbf{Generations of our model (\texttt{T = 1.00, max\_new\_tokens = 32, ppx = 141.78})}
\begin{enumerate}
\item \nThursday, July 15, 2017\nThe elegance is truly the unique, strong and well-timed arrays. Crustra Living is carved together together and
\item \u201d. But it\u2019s dominated michigan, the blue-horny top color blossoms, but basically is gorgeous such: Robin.\ufffd
\item dedicated to that power in the outdoors.\nOf course, another member of my girls Poems, not the US National Mineral Missions, an unhappy and immediateISH
\end{enumerate}



\textbf{Generations of Pythia (\texttt{T = 0.01, max\_new\_tokens = 32, ppx = 5.49})}
\begin{enumerate}
\item , the jungle is a place of mystery and danger. The jungle is a place of danger. The jungle is a place of danger. The jungle is a place
\item es are very cute, but they are not very smart. They are very curious and they are very playful. They are very playful and they are very curious.
\item been at war with the aliens for over a century. The aliens have been defeated, but the war is far from over. The aliens have been forced to retreat
\end{enumerate}


\textbf{Generations of Pythia (\texttt{T = 0.50, max\_new\_tokens = 32, ppx = 5.98})}
\begin{enumerate}
\item , in the heart of the jungle, in the heart of the jungle, in the heart of the jungle, in the heart of the jungle, in the heart
\item es.$\backslash$" $\backslash$"They're not as smart as they used to be.$\backslash$" $\backslash$"They're not as clever as they used to be.$\backslash$" $\backslash$"They're not as smart
\item reached the stage of civilisation where the old gods are no more. They are long gone and the world is ruled by the gods of war. The gods of
\end{enumerate}


\textbf{Generations of Pythia (\texttt{T = 1.00, max\_new\_tokens = 32, ppx = 21.31})}
\begin{enumerate}
\item , where the\nlumberjack's axe had cut through the bough, the bear saw the light.\nThe savage bear, whose jaws could swallow a
\item \nwho lived by the sea at Arromanches, had a very large head, and was\ncapable of throwing more than one stone at a time:
\item come under threat, as the Garmans move to eliminate all life at the cost of their lives. The Garmans have learned from the time they were
\end{enumerate}

\subsubsection{A}
\label{sec:org5f3fbb3}
\textbf{Prefixes}
\begin{enumerate}
\item In the heart of the jungle
\item The little red fox
\item The year is 2342. Humanity has
\end{enumerate}
\textbf{Hyperparameter Settings}: \(T \in \{0.01, 0.5, 1.0\}\), \texttt{max\_new\_tokens = 32}.
\subsubsection{B Effect of Temperature Increase}
\label{sec:orgc379fde}
Our model seems to make more sense closer to zero temperature. It also tends to be very repetitive at lower temperatures. Perplexity rises with temprature as expected.
\subsubsection{C Comparison at Same Temprature}
\label{sec:org7c68646}
\begin{center}
\begin{tabular}{ll}
Our Model & Pythia\\[0pt]
Is somewhat non-sensical at higher tempratures & Output is more sensible throughout\\[0pt]
Is very confident (low ppx) at low tempratures & Is less confident at low temprature\\[0pt]
Confidence drops sharply with temperature & Not as sharp a drop\\[0pt]
\end{tabular}
\end{center}
\textbf{Reasons}: Our model has seen less data and has much diminished capacity. Hence, the distribution it has learned is more concentrated on just a few next tokens for each input. This leads to:
\begin{enumerate}
\item False confidence at low tempratures.
\item Almost non-sensiscal output and low confidence at higher temperatures.
\end{enumerate}


\subsection{Question 1.3}
\label{sec:org25dc90d}
\subsubsection{A}
\label{sec:org12253df}
Perplexity of CMU Wiki paragraphs is 7.55 as is and 10.28 after removal of the/The. The word "The" is, relatively, an easy word to predict. In addition, it is a very common word. Thus, perplexity drops upon its removal.

\subsubsection{B}
\label{sec:org04a70a0}
Perplexity of Jabberwocky is 1.58 as is and 15.20 after substitution of gibberish words with gibberish words. The likely reason for this is that Pythia has alreads seen the original poem and its oddity is what gives its predictions such high confidence. It is like memorizing by rote. Given that it has memorized the poem very well, the distribution it has learned in context of this poem is very peaked and this leads to the very high perlexity when we change the non-words. 
\end{document}
