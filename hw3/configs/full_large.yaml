output_dir: outputs/full_large  # <- where the output files are written
input_file: tokens.npz
tokenizer_encoding: gpt2      # <- the tokenizer encoding, used by tiktoken (YOU SHOULD NOT CHANGE THIS)
model_config:
  n_embd: 256                  # <- dimension of token and positional embeddings 
  n_head: 8                   # <- number of attention heads in multihead attention
  n_positions: 64            # <- the maximum number of tokens that the model can take
  n_layer: 8                  # <- number of decoder blocks
device: auto                  # <- which device to put the model on (YOU DO NOT NEED TO CHANGE THIS)
batch_size: 256                # <- number of sequences to feed into the model at a time
seq_len: 64                  # <- length of each sequence in training and evaluation, <= model_config.n_positions
num_warmup_steps: 1259          # <- number of warmup steps in cosine annealing
num_training_steps: 12590      # <- number of training steps in cosine annealing
grad_accumulation_steps: 4    # <- number of micro steps of gradient accumulation before every model update
min_lr: 5e-4                  # <- minimum learning rate in cosine annealing
max_lr: 5e-3                  # <- maximum learning rate in cosine annealing
